{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint \n",
    "import math\n",
    "data=pd.read_csv('/mnt/c/Users/buzga/Desktop/School/grad_school/spring_2023/machine_learning/homework/hw2/hw2/ridge_regression_dataset.csv')\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Feature normalization\n",
    "def feature_normalization(train, test):\n",
    "    \"\"\"Rescale the data so that each feature in the training set is in\n",
    "    the interval [0,1], and apply the same transformations to the test\n",
    "    set, using the statistics computed on the training set.\n",
    "\n",
    "    Args:\n",
    "        train - training set, a 2D numpy array of size(num_instances, num_features)\n",
    "        test - test set, a 2D numpy array of size(num_instances, num_features)\n",
    "\n",
    "    Returns:\n",
    "        train_normalized - training set after normalization\n",
    "        test_normalized - test set after normalization\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    train_vals=[i for i in range(len(train[0,:])) if (np.all(train == train[0,:], axis = 0)[i]==False)] \n",
    "    test_vals=[i for i in range(len(test[0,:])) if (np.all(test == test[0,:], axis = 0)[i]==False)]\n",
    "    ## standardizes across non_constant rows. and discards constnat features. \n",
    "    train_numerator=(train[:,train_vals]- np.min(train[:,train_vals],axis=0))\n",
    "    train_denominator=(np.max(train[:,train_vals],axis=0)- np.min(train[:,train_vals],axis=0))\n",
    "    test_numeroatro=(test[:,test_vals]- np.min(test[:,test_vals],axis=0))\n",
    "    test_denomiator=(np.max(test[:,test_vals],axis=0)- np.min(test[:,test_vals],axis=0))\n",
    "    return train_numerator/train_denominator, test_numeroatro/test_denomiator\n",
    "\n",
    "\n",
    "#######################################\n",
    "### The square loss function\n",
    "def compute_square_loss(X, y, theta):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the average square loss for predicting y with X*theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        theta - the parameter vector, 1D array of size(num_features)\n",
    "\n",
    "    Returns:\n",
    "        loss - the average square loss, scalar\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    return np.dot((X@theta-y),(X@theta-y))/y.shape[0]\n",
    "\n",
    "\n",
    "#######################################\n",
    "### The gradient of the square loss function\n",
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the average square loss(as defined in compute_square_loss), at the point theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size(num_features)\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size(num_features)\n",
    "    \"\"\"\n",
    "    #TODO   \n",
    "    a=(X@theta)\n",
    "    b=a-y\n",
    "    return (2/y.shape[0])*X.T@b\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Gradient checker\n",
    "#Getting the gradient calculation correct is often the trickiest part\n",
    "#of any gradient-based optimization algorithm. Fortunately, it's very\n",
    "#easy to check that the gradient calculation is correct using the\n",
    "#definition of gradient.\n",
    "#See http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization\n",
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"Implement Gradient Checker\n",
    "    Check that the function compute_square_loss_gradient returns the\n",
    "    correct gradient for the given X, y, and theta.\n",
    "\n",
    "    Let d be the number of features. Here we numerically estimate the\n",
    "    gradient by approximating the directional derivative in each of\n",
    "    the d coordinate directions:\n",
    "(e_1 =(1,0,0,...,0), e_2 =(0,1,0,...,0), ..., e_d =(0,...,0,1))\n",
    "\n",
    "    The approximation for the directional derivative of J at the point\n",
    "    theta in the direction e_i is given by:\n",
    "(J(theta + epsilon * e_i) - J(theta - epsilon * e_i)) /(2*epsilon).\n",
    "\n",
    "    We then look at the Euclidean distance between the gradient\n",
    "    computed using this approximation and the gradient computed by\n",
    "    compute_square_loss_gradient(X, y, theta).  If the Euclidean\n",
    "    distance exceeds tolerance, we say the gradient is incorrect.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size(num_features)\n",
    "        epsilon - the epsilon used in approximation\n",
    "        tolerance - the tolerance error\n",
    "\n",
    "    Return:\n",
    "        A boolean value indicating whether the gradient is correct or not\n",
    "    \"\"\"\n",
    "    true_gradient = compute_square_loss_gradient(X, y, theta) #The true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    approx_grad= list(map(lambda I:( compute_square_loss(X,y,theta + epsilon * I)-compute_square_loss(X,y,theta - epsilon * I) )/(2*epsilon),np.identity(num_features)))\n",
    "    distances=true_gradient-approx_grad\n",
    "    euclidian_distance=(distances.T)@(distances)\n",
    "    return euclidian_distance<=tolerance\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Generic gradient checker\n",
    "def generic_gradient_checker(X, y, theta, objective_func, gradient_func, \n",
    "                             epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    The functions takes objective_func and gradient_func as parameters. \n",
    "    And check whether gradient_func(X, y, theta) returned the true \n",
    "    gradient for objective_func(X, y, theta).\n",
    "    Eg: In LSR, the objective_func = compute_square_loss, and gradient_func = compute_square_loss_gradient\n",
    "    \"\"\"\n",
    "    true_gradient = gradient_func(X, y, theta) #The true gradient\n",
    "    #print(\"true gradinet is \", true_gradient)\n",
    "    num_features = theta.shape[0]\n",
    "    # approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    approx_grad= list(map(lambda I:( objective_func(X,y,theta + epsilon * I)-objective_func(X,y,theta - epsilon * I) )/(2*epsilon),np.identity(num_features)))\n",
    "    #print(\"print estimated gradient is\", approx_grad)\n",
    "    distances=true_gradient-approx_grad\n",
    "    euclidian_distance=(distances.T)@(distances)\n",
    "    #print(\"there euclidian distance is, \", euclidian_distance)\n",
    "    result=euclidian_distance<=tolerance\n",
    "    #print(\"thus is is {0} that the true and aproximate gradients are within tollerence\".format(result))\n",
    "    return euclidian_distance<=tolerance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Batch gradient descent\n",
    "def batch_grad_descent(X, y, alpha=0.1, num_step=1000, grad_check=False):\n",
    "    \"\"\"\n",
    "    In this question you will implement batch gradient descent to\n",
    "    minimize the average square loss objective.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        num_step - number of steps to run\n",
    "        grad_check - a boolean value indicating whether checking the gradient when updating\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size(num_step+1, num_features)\n",
    "                     for instance, theta in step 0 should be theta_hist[0], theta in step(num_step) is theta_hist[-1]\n",
    "        loss_hist - the history of average square loss on the data, 1D numpy array,(num_step+1)\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_step + 1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step + 1)  #Initialize loss_hist\n",
    "    theta = np.zeros(num_features)  #Initialize theta\n",
    "    i=0\n",
    "    while i<num_step:\n",
    "        current_grad=compute_square_loss_gradient(X, y, theta)\n",
    "        theta=theta-(alpha*current_grad)\n",
    "        theta_hist[i+1]=theta\n",
    "        loss_hist[i+1]=compute_square_loss(X,y,theta)\n",
    "        i=i+1\n",
    "    return theta_hist,loss_hist\n",
    "    #TODO\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "### The gradient of regularized batch gradient descent\n",
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute the gradient of L2-regularized average square loss function given X, y and theta\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size(num_features)\n",
    "        lambda_reg - the regularization coefficient\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size(num_features)\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    a=(X@theta)\n",
    "    b=a-y\n",
    "    return (2/y.shape[0])*X.T@b+2*lambda_reg*theta\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Regularized batch gradient descent\n",
    "def regularized_grad_descent(X, y, alpha=0.05, lambda_reg=10**-2, num_step=1000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        lambda_reg - the regularization coefficient\n",
    "        num_step - number of steps to run\n",
    "    \n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size(num_step+1, num_features)\n",
    "                     for instance, theta in step 0 should be theta_hist[0], theta in step(num_step+1) is theta_hist[-1]\n",
    "        loss hist - the history of average square loss function without the regularization term, 1D numpy array.\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_step + 1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step + 1)  #Initialize loss_hist\n",
    "    theta = np.zeros(num_features)  #Initialize theta\n",
    "    i=0\n",
    "    while i<num_step:\n",
    "        current_grad=compute_regularized_square_loss_gradient(X, y, theta,lambda_reg)\n",
    "        theta=theta-(alpha*current_grad)\n",
    "        theta_hist[i+1]=theta\n",
    "        loss_hist[i+1]=compute_square_loss(X,y,theta)\n",
    "        i=i+1\n",
    "    return theta_hist,loss_hist\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Stochastic gradient descent\n",
    "\n",
    "def stochastic_grad_descent(X, y, alpha=0.01, lambda_reg=10**-2, num_epoch=1000, eta0=False,c=None):\n",
    "    \"\"\"\n",
    "    In this question you will implement stochastic gradient descent with regularization term\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        alpha - string or float, step size in gradient descent\n",
    "                NOTE: In SGD, it's not a good idea to use a fixed step size. Usually it's set to 1/sqrt(t) or 1/t\n",
    "                if alpha is a float, then the step size in every step is the float.\n",
    "                if alpha == \"1/sqrt(t)\", alpha = 1/sqrt(t).\n",
    "                if alpha == \"1/t\", alpha = 1/t.\n",
    "        lambda_reg - the regularization coefficient\n",
    "        num_epoch - number of epochs to go through the whole training set\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 3D numpy array of size(num_epoch, num_instances, num_features)\n",
    "                     for instance, theta in epoch 0 should be theta_hist[0], theta in epoch(num_epoch) is theta_hist[-1]\n",
    "        loss hist - the history of loss function vector, 2D numpy array of size(num_epoch, num_instances)\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta = np.ones(num_features) # Initialize theta\n",
    "    theta_hist = np.zeros((num_epoch, num_instances, num_features)) # Initialize t\n",
    "    loss_hist = np.zeros((num_epoch, num_instances)) # Initialize loss_hist\n",
    "    random = np.random.default_rng()\n",
    "    index = np.arange(X.shape[0])\n",
    "    for i in range(1, num_epoch + 1):\n",
    "        random.shuffle(index)\n",
    "        for j, (x, y_) in enumerate(zip(X[index], y[index])):\n",
    "            if alpha == 'C/sqrt(t)':\n",
    "                eta = c / np.sqrt(i+1) \n",
    "            elif alpha=='1/t':\n",
    "                eta=c/(i+1)\n",
    "            else:\n",
    "                eta=alpha\n",
    "        calc_theta=lambda theta:(x.T * (x.T @ theta - y_) +2 * lambda_reg * theta)\n",
    "        theta = theta - 2*eta*calc_theta(theta)\n",
    "        theta_hist[i - 1, j] = theta\n",
    "        loss_hist[i - 1, j] = compute_square_loss(X, y, theta)\n",
    "    return theta_hist,loss_hist,\n",
    "\n",
    "def load_data():\n",
    "    #Loading the dataset\n",
    "    print('loading the dataset')\n",
    "\n",
    "    df = pd.read_csv('ridge_regression_dataset.csv', delimiter=',')\n",
    "    X = df.values[:,:-1]\n",
    "    y = df.values[:,-1]\n",
    "\n",
    "    print('Split into Train and Test')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, random_state=10)\n",
    "\n",
    "    print(\"Scaling all to [0, 1]\")\n",
    "    X_train, X_test = feature_normalization(X_train, X_test)\n",
    "    X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))  # Add bias term\n",
    "    X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1))))\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test= load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question 13 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [.05,.025, .01]:\n",
    "    a,b=batch_grad_descent(X_train, y_train, alpha=i, grad_check=True)\n",
    "    #print(\"hi\")\n",
    "    #print(b)\n",
    "    t = np.arange(1001)\n",
    "    plt.plot(t,b,label=\"alpha={0}\".format(i))\n",
    "    plt.xlabel(\"iteration number\")\n",
    "    #plt.xscale('log')\n",
    "    #plt.yscale('log')\n",
    "    plt.ylabel(\"average suare loss\")\n",
    "    plt.grid() \n",
    "    plt.title(\"average training loss for difrent learning rates (alpha)\")\n",
    "    plt.legend()\n",
    "plt.show()\n",
    "for i in [.1]:\n",
    "    a,b=batch_grad_descent(X_train, y_train, alpha=i, grad_check=True)\n",
    "    #print(\"hi\")\n",
    "    #print(b)\n",
    "    t = np.arange(1001)\n",
    "    plt.plot(t,b,label=\"alpha={0}\".format(i))\n",
    "    plt.xlabel(\"iteration number\")\n",
    "    #plt.xscale('log')\n",
    "    #plt.yscale('log')\n",
    "    plt.ylabel(\"average suare loss\")\n",
    "    plt.grid() \n",
    "    plt.title(\"average training loss for difrent learning rates (alpha)\")\n",
    "    plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_for_batch_grad_descent(X_train, y_train,X_test,y_test, alpha=0.1, num_step=1000, grad_check=False):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        num_step - number of steps to run\n",
    "        grad_check - a boolean value indicating whether checking the gradient when updating\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size(num_step+1, num_features)\n",
    "                     for instance, theta in step 0 should be theta_hist[0], theta in step(num_step) is theta_hist[-1]\n",
    "        loss_hist - the history of average square loss on the data, 1D numpy array,(num_step+1)\n",
    "    \"\"\"\n",
    "    i=0\n",
    "    num_instances, num_features = X_train.shape[0], X_train.shape[1]\n",
    "    theta_hist = np.zeros((num_step + 1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step + 1)  #Initialize loss_hist\n",
    "    theta = np.zeros(num_features)  #Initialize theta\n",
    "    while i<num_step:\n",
    "            current_grad=compute_square_loss_gradient(X_train,y_train, theta)\n",
    "            theta=theta-(alpha*current_grad)\n",
    "            theta_hist[i+1]=theta\n",
    "            loss_hist[i+1]=compute_square_loss(X_test,y_test,theta)\n",
    "            i=i+1\n",
    "    return theta_hist,loss_hist\n",
    "\n",
    "for i in [.05,.025, .01]:\n",
    "    a,b=test_loss_for_batch_grad_descent(X_train, y_train,X_test,y_test, alpha=i,grad_check=True)\n",
    "    t = np.arange(1001)\n",
    "    plt.plot(t,b,label=\"alpha={0}\".format(i))\n",
    "    plt.xlabel(\"iteration number\")\n",
    "    #plt.xscale('log')\n",
    "    #plt.yscale('log')\n",
    "    plt.ylabel(\"average suare loss\")\n",
    "    plt.grid() \n",
    "    plt.legend()\n",
    "plt.title(\"average test loss for difrent learning rates (alpha)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [math.pow(10,-5),math.pow(10,-3),.1,1 ,10 ]:\n",
    "    a,b=regularized_grad_descent(X_train, y_train, alpha=.05,lambda_reg=i)\n",
    "    t = np.arange(1001)\n",
    "    plt.plot(t,b,label=\"lambda={0}\".format(i))\n",
    "    plt.xlabel(\"iteration number\")\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel(\"average suare loss\")\n",
    "    plt.grid() \n",
    "    plt.title(\"average training loss for difrent coefcients of regularization (lambda)\")\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_for_regularized_grad_descent(X_train, y_train,X_test,y_test, alpha=0.1, num_step=1000, grad_check=False,lambda_reg=10**-2):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        num_step - number of steps to run\n",
    "        grad_check - a boolean value indicating whether checking the gradient when updating\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size(num_step+1, num_features)\n",
    "                     for instance, theta in step 0 should be theta_hist[0], theta in step(num_step) is theta_hist[-1]\n",
    "        loss_hist - the history of average square loss on the data, 1D numpy array,(num_step+1)\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X_train.shape[0], X_train.shape[1]\n",
    "    theta_hist = np.zeros((num_step + 1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step + 1)  #Initialize loss_hist\n",
    "    theta = np.zeros(num_features)  #Initialize theta\n",
    "    for i in range(num_step):\n",
    "        current_grad=compute_regularized_square_loss_gradient(X_train, y_train, theta, lambda_reg)\n",
    "        theta=theta-(alpha*current_grad)\n",
    "        theta_hist[i+1]=theta\n",
    "        loss_hist[i+1]=compute_square_loss(X_test,y_test,theta)\n",
    "\n",
    "    return theta_hist,loss_hist\n",
    "for i in [math.pow(10,-7),math.pow(10,-5),math.pow(10,-3),.01,1 ,10 ]:\n",
    "    c,d=regularized_grad_descent(X_train, y_train, alpha=.05,lambda_reg=i)\n",
    "    a,b=test_loss_for_regularized_grad_descent(X_train, y_train,X_test, y_test, alpha=.05,lambda_reg=i)\n",
    "    t = np.arange(1000)\n",
    "    plt.plot(t,d[1:],label=\"training loss\")\n",
    "    plt.plot(t,b[1:],label=\"test loss\")\n",
    "    plt.xlabel(\"iteration number\")\n",
    "    #plt.xscale('log')\n",
    "    #plt.yscale('log')\n",
    "    plt.ylabel(\"average suare loss\")\n",
    "    plt.grid() \n",
    "    plt.title(\"average training and test loss for lambda={0}\".format(i))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 18\n",
    "temp=[]\n",
    "lambdas=[.0001,.001,.01,.1]\n",
    "for i in lambdas:\n",
    "    a,b=regularized_grad_descent(X_train, y_train, alpha=.05,lambda_reg=i)\n",
    "    temp.append(b[-1])\n",
    "plt.plot(lambdas,temp,label=\"lambda={0}\".format(i))\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.ylabel(\"average suare loss\")\n",
    "plt.grid() \n",
    "plt.title(\"final training loss for difrent coefcients of regularization (lambda)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 18\n",
    "temp=[[],[]]\n",
    "lambdas=np.linspace(.0001,.1,100)\n",
    "for i in lambdas:\n",
    "    c,d=regularized_grad_descent(X_train, y_train, alpha=.05,lambda_reg=i)\n",
    "    a,b=test_loss_for_regularized_grad_descent(X_train, y_train,X_test, y_test, alpha=.05,lambda_reg=i)\n",
    "    temp[0].append(b[-1])\n",
    "    temp[1].append(d[-1])\n",
    "plt.plot(lambdas,temp[0], label=\"testing error\")\n",
    "plt.plot(lambdas,temp[1],label=\"Training error\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.ylabel(\"average suare loss\")\n",
    "plt.grid() \n",
    "plt.title(\"final test loss for difrent coefcients of regularization (lambda)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(X_train, y_train,X_test,y_test, alpha=0.1, num_step=1000, grad_check=False,lambda_reg=10**-2):\n",
    "    num_instances, num_features = X_train.shape[0], X_train.shape[1]\n",
    "    training_loss_hist = np.zeros(num_step + 1)  #Initialize loss_hist\n",
    "    testing_loss_hist = np.zeros(num_step + 1)  #Initialize loss_hist\n",
    "    theta = np.zeros(num_features)  #Initialize theta\n",
    "    i=0\n",
    "    while i<=num_step:\n",
    "        current_grad=compute_regularized_square_loss_gradient(X_train, y_train, theta, lambda_reg)\n",
    "        theta=theta-(alpha*current_grad)\n",
    "        training_loss_hist[i]=compute_square_loss(X_test,y_test,theta)\n",
    "        testing_loss_hist[i]=compute_square_loss(X_test,y_test,theta)\n",
    "        i=i+1\n",
    "    return np.min(testing_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[[],[]]\n",
    "early_stops=[]\n",
    "lambdas=np.linspace(.0001,.1,110)\n",
    "for i in lambdas:\n",
    "    a,b=test_loss_for_regularized_grad_descent(X_train, y_train,X_test, y_test, alpha=.05,lambda_reg=i)\n",
    "    c=early_stopping(X_train, y_train,X_test, y_test, alpha=.05,lambda_reg=i)\n",
    "    e,d=regularized_grad_descent(X_train, y_train, alpha=.05,lambda_reg=i)\n",
    "    early_stops.append(c)\n",
    "    temp[0].append(b[-1])\n",
    "    temp[1].append(d[-1])\n",
    "plt.plot(lambdas,temp[0], label=\"testing error\")\n",
    "plt.plot(lambdas,temp[1],label=\"Training error\")\n",
    "plt.plot(lambdas,early_stops, label=\"early stopping testing error\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.ylabel(\"average suare loss\")\n",
    "plt.grid() \n",
    "plt.title(\"early stopping vs final test erorr for difrent coefcients of regularization (lambda)\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qestion 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def pre_process_mnist_01():\n",
    "    \"\"\"\n",
    "    Load the mnist datasets, selects the classes 0 and 1 \n",
    "    and normalize the data.\n",
    "    Args: none\n",
    "    Outputs: \n",
    "        X_train: np.array of size (n_training_samples, n_features)\n",
    "        X_test: np.array of size (n_test_samples, n_features)\n",
    "        y_train: np.array of size (n_training_samples)\n",
    "        y_test: np.array of size (n_test_samples)\n",
    "    \"\"\"\n",
    "    X_mnist, y_mnist = fetch_openml('mnist_784', version=1, \n",
    "                                    return_X_y=True, as_frame=False)\n",
    "    indicator_01 = (y_mnist == '0') + (y_mnist == '1')\n",
    "    X_mnist_01 = X_mnist[indicator_01]\n",
    "    y_mnist_01 = y_mnist[indicator_01]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_mnist_01, y_mnist_01,\n",
    "                                                        test_size=0.33,\n",
    "                                                        shuffle=False)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train) \n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    y_test = 2 * np.array([int(y) for y in y_test]) - 1\n",
    "    y_train = 2 * np.array([int(y) for y in y_train]) - 1\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def sub_sample(N_train, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Subsample the training data to keep only N first elements\n",
    "    Args: none\n",
    "    Outputs: \n",
    "        X_train: np.array of size (n_training_samples, n_features)\n",
    "        X_test: np.array of size (n_test_samples, n_features)\n",
    "        y_train: np.array of size (n_training_samples)\n",
    "        y_test: np.array of size (n_test_samples)\n",
    "    \"\"\"\n",
    "    assert N_train <= X_train.shape[0]\n",
    "    return X_train[:N_train, :], y_train[:N_train]\n",
    "\n",
    "# def classification_error(clf, X, y):\n",
    "    ## TODO\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = pre_process_mnist_01()\n",
    "\n",
    "clf = SGDClassifier(loss='log', max_iter=1000, \n",
    "                    tol=1e-3,\n",
    "                    penalty='l1', alpha=0.01, \n",
    "                    learning_rate='invscaling', \n",
    "                    power_t=0.5,                \n",
    "                    eta0=0.01,\n",
    "                    verbose=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# test = classification_error(clf, X_test, y_test)\n",
    "# train = classification_error(clf, X_train, y_train)\n",
    "# print('train: ', train, end='\\t')\n",
    "# print('test: ', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = pre_process_mnist_01()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(X_train[5], (28,28)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_error(estimator,observations, lables):\n",
    "    preds=estimator.predict(observations)\n",
    "    return np.sum(preds!=lables)/len(lables)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train=sub_sample(100, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "report the test classification error achieved by the logistic regression as a function of the\n",
    "regularization parameters α (taking 10 values between 10−4 and 10−1). You should make a\n",
    "plot with α as the x-axis in log scale. For each value of α, you should repeat the experiment\n",
    "10 times so has to finally report the mean value and the standard deviation. You should\n",
    "use plt.errorbar to plot the standard deviation as error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas=np.linspace(10**-4,.2,20)\n",
    "#out=[[],[],[]]\n",
    "out=dict()\n",
    "for alpha in alphas:\n",
    "    temp=[]\n",
    "    clf = SGDClassifier(loss='log', max_iter=1000, \n",
    "                    tol=1e-3,\n",
    "                    penalty='l1', alpha=alpha, \n",
    "                    learning_rate='invscaling', \n",
    "                    power_t=0.5,                \n",
    "                    eta0=0.01,\n",
    "                    verbose=1)\n",
    "    for i in range(10):\n",
    "        clf.fit(X_train, y_train)\n",
    "        temp.append(classification_error(clf, X_test,y_test))\n",
    "    out[alpha]=temp\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    plt.scatter(alpha, np.mean(out[alpha]))\n",
    "    plt.errorbar(alpha, np.mean(out[alpha]),\n",
    "             yerr = np.std(out[alpha]),\n",
    "             fmt ='o')\n",
    "    plt.xlabel(\"alpha value\")\n",
    "    plt.ylabel(\"classfication error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas=np.linspace(10**-4,10**-1,10)\n",
    "#out=[[],[],[]]\n",
    "out=dict()\n",
    "scales=dict()\n",
    "for alpha in alphas:\n",
    "    temp=[]\n",
    "    clf = SGDClassifier(loss='log', max_iter=1000, \n",
    "                    tol=1e-3,\n",
    "                    penalty='l1', alpha=alpha, \n",
    "                    learning_rate='invscaling', \n",
    "                    power_t=0.5,                \n",
    "                    eta0=0.01,\n",
    "                    verbose=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    out[alpha]=np.reshape(clf.coef_, (28,28))\n",
    "    scales[alpha]=np.abs(clf.coef_).max()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    plt.imshow(out[alpha],cmap=plt.cm.RdBu, vmax=scales[alpha], vmin=-scales[alpha])\n",
    "    plt.title(\"alpha={0}\".format(alpha))\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "theta_hist, loss_hist = stochastic_grad_descent(X_train, y_train, alpha='1/t', c=.01, )\n",
    "x=np.linspace(0,999,1000)\n",
    "plt.plot(loss_hist[:,-1], ms=1,label=\"1/t\")\n",
    "theta_hist, loss_hist = stochastic_grad_descent(X_train, y_train, alpha='C/sqrt(t)', c=.01)\n",
    "x=np.linspace(0,999,1000)\n",
    "plt.plot(loss_hist[:,-1], ms=1, label=\"1/sqrt(t)\")\n",
    "plt.yscale(\"log\")\n",
    "#plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad1d31a919d462a2e3b4536d0f8b2c01def87898c36c77ad49eb54edfe3bbd3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
