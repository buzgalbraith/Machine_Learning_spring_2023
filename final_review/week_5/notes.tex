\documentclass{article}
\usepackage[utf8]{inputenc}
\title{Lecture 10: Kernel methods}
\author{wbg231 }
\date{December 2022}
\newcommand{\R}{$\mathbb{R}$}
\newcommand{\B}{$\beta$}
\newcommand{\A}{$\alpha$}
\newcommand{\D}{\Delta}

\newcommand{\avector}[2]{(#1_2,\ldots,#1_{#2})}
\newcommand{\makedef}[2]{$\textbf{#1}$:#2 }
\usepackage{tikz,graphicx,hyperref,amsmath,amsfonts,amscd,amssymb,bm,cite,epsfig,epsf,url}

\begin{document}

\maketitle

\section{feature maps }
\begin{itemize}
\item \textcolor{blue}{explicit feature map for linear model} let $\phi:X\rightarrow\mathbb{R}^{d}$ be a feature and our hypothsis space be the affine functions on that feature space $$\mathcal{F}-\{x\rightarrow w^{t}\phi(x)+b|w\in \mathbb{R}^{D}, b\in \mathbb{R}\}$$
\item the idea is that we identify some feature space, which we can map our features onto to make the classes linearly seperable \\ \includegraphics*[width=10cm]{images/Screenshot 2023-05-12 at 3.56.50 PM.png}
\item  adding more feature, allows our linear models to become more expressive
\item we can either explicitly specicfy feature but this requires domain knowledge or think of our features as building blocks that can be put together
\item we can not do this naively however as adding more features to our representation leads our data matrix to become very large causes overfitting and memory issues 
\item we can try to avoid overfitting with regularization and cross validation 
\item we can try to solve computational or memory issues with kernel methods 
\subsection*{kernel methods }
\item \textcolor{blue}{svm objective with an explicit feature map} can be expressed as $$j(w)=\frac{1}{2}||w||_{2}^2+\frac{c}{n}\sum_{i=1}^{n}max(0,1-y_iw^{t}\phi(x_i))$$
\item this yields the following dual problem $$max \sum_{i}\alpha_{i}-\frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_j<\phi(x_j),\phi(x_i)>$$ $$st\quad \sum_{i}\alpha_iy_i=0, \alpha_{i}\in [0,\frac{c}{n}]$$
\item note that $\phi(x)$ our feature map only shows up in inner products in both our training and and observations 
\item note that we can calculate the inner product of our data tansformed in the feature space with out actually touching the feature space this means our computational cost goes down dramatically
\subsection*{kernel function}
\item we define a kernel function as the inner product of two feature maps over some hilbert space $$k(x,x')=<\phi(x),\phi(x')>$$
\item \textcolor{red}{a method is kernelized} if every feature vector $\phi(x)$  only appears inside an inner product with another feature vector $\phi(x')$. this must hold for both the optimization problem and prediction function
\item call the kernel matrix a matrix such that each enterry is the kernel of the coresponding enteries in the orignal data matrix that is $K_{i,j}=k(x_i,x_j)$
\item the kernel matrix summarizes all information across the training inputs $x_1\cdots x_n$ that are required to solve teh optimization problem
\item when we kernelized an algorithm we can swap out the inner product, for a new kernel function that may correspond to a very high feature space
\item there is an upfront cost of computating the kernel matrix of $O(d)$ but once we have computer that kernel the computaitonal cost of prediction depends on the number of data points not the sie of the feature space
\item think of kernel as similarity scores in some hilbert space 
\item a symmetric function is positive definite if the kernel matrix generated by that fucntion is positive semi defininte for all potnetial sets of inputs
\item note that a symmetric function can be expressed as an inner product of some feature map $\iff$ if the kernel function is PD
\item can modify old kernel to make new ones 
\item linear kernel is just $K(x,x')=x^{t}x'$
\item quadratic kernel $<\phi(x), \phi(x')>=<x,x'>+<x,x'>^{2}$ 
\subsection*{representer theorem}
\item notice that in svm our optimal parameter is $$\phi^{*}=\sum_{i}\alpha_i^{*}y_ix_i=\sum_i\gamma_ix_i\iff w^{*}\in span(X)$$
\item ridge regression has the closed form solution $(X^TX+\lambda I)^{-1}X^{t}y$ 
\item   note that we can write $sw^{*}=X^{T}(\frac{1}{\lambda } y -\frac{1}{\lambda } Xw^{*})=X^{t}\alpha^{*}\iff w^{*}\in span(X)$
\item so note that our orignal ridge regression $$w^{*}=argmin_{w\in \mathbb{R}^{d}}\sum_{i=1}^{n}(w^tx_i+y_i)^{2}+\lambda||w||^{2}$$ given the fact that $w"\in span(X)$ , we cna just mnimize over that span(x)
that is $$w^{*}=argmin_{w\in span(X)}\sum_{i=1}^{n}(w^tx_i+y_i)^{2}+\lambda||w||^{2}=argmin_{\alpha\in \mathbb{R}^{n}}\frac{1}{n}\sum_{i=1}^{n}((X^T\alpha)^tx_i -y_i)^{2}+\lambda ||X^T\alpha||_2^{2}$$ 
\item so we have taken our search space from $\mathbb{R}^{d}$ to $\mathbb{R}^{n}$
\section*{General objective function over linear hypothesis space}
\item given $w,x_1.\\cdots x_n \in \mathcal{H}$ from some hilbert space 
\item where the norm $||*||$ is the norm of the corresponding inner product space
\item R is a regularization temr 
\item and L is a loss term 
\item we can write our generalized linear objective as $$j(w)=min_{w\in \mathbb{H}}R(||w||)+L(<w,x_1> \cdots <w,x_n>)$$ 
\item \textcolor{blue}{Representer theorem} any function that can fit into the form $$j(w)=min_{w\in \mathbb{H}}R(||w||)+L(<w,x_1> \cdots <w,x_n>)$$  ie generalized linear objective will have a minimizer w such that $w\in span(X)$
\item if we define$ K=X^TX$ and write $w=\sum_{i}^{n}\alpha_i x_i $ than we can re-write our objective function in terms of our kernel matrix K as $$J_{0}(\alpha)=R(||\sum_{i=1}^{n}\alpha_{i}x_i||) + L(s(\sum_{i}\alpha_ix_i))=R(\sqrt{\alpha^{t}K\alpha})+L(K\alpha)$$
\item  which we can mnimize over $\alpha\in \mathbb{R}^{n}$ and our prediction function can be expressed as $\hat{f}=<w^{*}, x> =<\sum_{i=1}^{n}\alpha_ix_i, x_j> =\sum_{i=1}^{n}<x_i,x_j>=\alpha K$
\item by the recomender theorem all norm-regualrized mdoels can be kernalized 
\item so this tells us that all info we need about our data X is Summarized in it's gram matrix  
\end{itemize}

\end{document}